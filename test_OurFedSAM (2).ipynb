{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOAIh8jkImeI",
        "outputId": "b9566db6-a17e-48b5-cf09-247c7954be14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tqdm as tqdm\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from typing import Type\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "!pip install tensorboardX\n",
        "import tensorboardX\n",
        "from tensorboardX import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT1lvMrJayFp"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "def args_parser():\n",
        "\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f')\n",
        "\n",
        "    # federated arguments (Notation for the arguments followed from paper)\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=10,\n",
        "                        help=\"number of rounds of training\")\n",
        "    parser.add_argument('--num_users', type=int, default=100,\n",
        "                        help=\"number of users: K\")\n",
        "    parser.add_argument('--frac', type=float, default=0.1,\n",
        "                        help='the fraction of clients: C')\n",
        "    parser.add_argument('--local_ep', type=int, default=2,\n",
        "                        help=\"the number of local epochs: E\")\n",
        "    parser.add_argument('--local_bs', type=int, default=128,\n",
        "                        help=\"local batch size: B\")\n",
        "    parser.add_argument('--lr', type=float, default=0.1,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--momentum', type=float, default=0.99,\n",
        "                        help='SGD momentum (default: 0.5)')\n",
        "\n",
        "\n",
        "    # model arguments\n",
        "    parser.add_argument('--model', type=str, default='resnet', help='model name')\n",
        "    parser.add_argument('--kernel_num', type=int, default=9,\n",
        "                        help='number of each kind of kernel')\n",
        "    parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
        "                        help='comma-separated kernel size to \\\n",
        "                        use for convolution')\n",
        "    parser.add_argument('--num_channels', type=int, default=1, help=\"number \\\n",
        "                        of channels of imgs\")\n",
        "    parser.add_argument('--norm', type=str, default='batch_norm',\n",
        "                        help=\"batch_norm, layer_norm, or None\")\n",
        "    parser.add_argument('--num_filters', type=int, default=32,\n",
        "                        help=\"number of filters for conv nets -- 32 for \\\n",
        "                        mini-imagenet, 64 for omiglot.\")\n",
        "    parser.add_argument('--max_pool', type=str, default='True',\n",
        "                        help=\"Whether use max pooling rather than \\\n",
        "                        strided convolutions\")\n",
        "\n",
        "\n",
        "    # other arguments\n",
        "    parser.add_argument('--dataset', type=str, default='cifar', help=\"name \\\n",
        "                        of dataset\")\n",
        "    parser.add_argument('--num_classes', type=int, default=10, help=\"number \\\n",
        "                        of classes\")\n",
        "    parser.add_argument('--gpu_id', default=None, help=\"To use cuda, set \\\n",
        "                        to a specific GPU ID. Default set to use CPU.\")\n",
        "    parser.add_argument('--optimizer', type=str, default='adam', help=\"type \\\n",
        "                        of optimizer\")\n",
        "    parser.add_argument('--iid', type=int, default=1,\n",
        "                        help='Default set to IID. Set to 0 for non-IID.')\n",
        "    parser.add_argument('--unequal', type=int, default=0,\n",
        "                        help='whether to use unequal data splits for  \\\n",
        "                        non-i.i.d setting (use 0 for equal splits)')\n",
        "    parser.add_argument('--stopping_rounds', type=int, default=10,\n",
        "                        help='rounds of early stopping')\n",
        "    parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
        "    parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU3Zk0lPa0h7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
        "        x = self.layer_input(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "class CNNMnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNMnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class CNNFashion_Mnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNFashion_Mnist, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class CNNCifar(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNCifar, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class modelC(nn.Module):\n",
        "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
        "        super(AllConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, 96, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2)\n",
        "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2)\n",
        "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(192, 192, 1)\n",
        "\n",
        "        self.class_conv = nn.Conv2d(192, n_classes, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_drop = F.dropout(x, .2)\n",
        "        conv1_out = F.relu(self.conv1(x_drop))\n",
        "        conv2_out = F.relu(self.conv2(conv1_out))\n",
        "        conv3_out = F.relu(self.conv3(conv2_out))\n",
        "        conv3_out_drop = F.dropout(conv3_out, .5)\n",
        "        conv4_out = F.relu(self.conv4(conv3_out_drop))\n",
        "        conv5_out = F.relu(self.conv5(conv4_out))\n",
        "        conv6_out = F.relu(self.conv6(conv5_out))\n",
        "        conv6_out_drop = F.dropout(conv6_out, .5)\n",
        "        conv7_out = F.relu(self.conv7(conv6_out_drop))\n",
        "        conv8_out = F.relu(self.conv8(conv7_out))\n",
        "\n",
        "        class_out = F.relu(self.class_conv(conv8_out))\n",
        "        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        return pool_out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# building a ResNet18 Architecture\n",
        "#this function creates a 3x3 convolutional layer with specified input and output channels, stride, padding, and without bias\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHwhybC2a2wo",
        "outputId": "631febc9-500a-4193-cd9a-b532b73e75c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newest\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import copy\n",
        "\n",
        "print(\"newest\")\n",
        "class DatasetSplit(Dataset):\n",
        "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = [int(i) for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return torch.tensor(image), torch.tensor(label)\n",
        "\n",
        "\n",
        "\n",
        "class LocalUpdate(object):\n",
        "    def __init__(self, args, dataset, idxs, logger, r):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "        self.trainloader, self.validloader, self.testloader, self.sample_size = self.train_val_test(\n",
        "            dataset, list(idxs))\n",
        "        self.r = r\n",
        "        # self.device = 'cuda' if args.gpu else 'cpu'\n",
        "        self.device = 'cpu'\n",
        "        # Default criterion set to NLL loss function\n",
        "        self.criterion = nn.NLLLoss().to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train_val_test(self, dataset, idxs):\n",
        "\n",
        "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
        "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
        "        idxs_test = idxs[int(0.9*len(idxs)):]\n",
        "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
        "                                 batch_size=self.args.local_bs, shuffle=True)\n",
        "        validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
        "                                 batch_size=self.args.local_bs, shuffle=False)\n",
        "        testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
        "                                 batch_size=self.args.local_bs, shuffle=False)\n",
        "\n",
        "        # validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
        "        #                          batch_size=int(len(idxs_val)/10), shuffle=False)\n",
        "        # testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
        "        #                         batch_size=int(len(idxs_test)/10), shuffle=False)\n",
        "        sample_size = len(trainloader.dataset)\n",
        "        return trainloader, validloader, testloader, sample_size\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_gradients(self, model):\n",
        "        # Method to calculate gradients of the model's parameters after JUST ONE epoch\n",
        "        model.train()\n",
        "\n",
        "        # Create a SGD optimizer for gradient calculation\n",
        "        #optimizer = torch.optim.SGD(model.parameters(), lr=self.args.lr, momentum=0.9, weight_decay = 1e-4)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = self.args.lr, weight_decay = 1e-4)\n",
        "\n",
        "        for iter in range(1):  # Iterate for one epoch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                # Forward pass and calculate loss\n",
        "                log_probs = model(images)\n",
        "                loss = self.criterion(log_probs, labels)\n",
        "                loss.backward()  # Backpropagate and calculate gradients\n",
        "\n",
        "                optimizer.step()  # Update parameters using gradients\n",
        "\n",
        "\n",
        "        total_batches = len(self.trainloader)\n",
        "        local_grad = [p.grad.clone() / total_batches for p in model.parameters()]\n",
        "        #print(local_grad)\n",
        "        #print(f'Gradient shape: {local_grad.size()}')\n",
        "        return local_grad\n",
        "\n",
        "        # total_grads = len(local_grad)\n",
        "        # avg_grads = [torch.zeros_like(grad) for grad in local_grad[0]]  # Initialize with zeros\n",
        "\n",
        "        # for i in range(total_grads):\n",
        "\n",
        "        #     avg_grads += local_grad[i]\n",
        "\n",
        "        # avg_grads/= total_grads\n",
        "\n",
        "        # return avg_grads\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def update_weights(self, model, global_round, r, p):\n",
        "    #     model.train()\n",
        "    #     optimizer = SAM(model.parameters(), base_optimizer=torch.optim.SGD, rho=0.03, lr=self.args.lr, momentum=0.5)\n",
        "\n",
        "    #     #avg_grads = calculate_average_gradient([self.calculate_gradients(model) for _ in range(self.args.local_ep)])\n",
        "\n",
        "    #     for iter in range(self.args.local_ep):\n",
        "    #         optimizer.zero_grad()\n",
        "\n",
        "    #         for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
        "    #             images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "    #             # Forward pass and calculate loss\n",
        "    #             log_probs = model(images)\n",
        "    #             loss = self.criterion(log_probs, labels)\n",
        "    #             loss.backward()  # Backpropagate and calculate gradients\n",
        "\n",
        "    #             # Calculate cosine similarity between local gradients and other gradients\n",
        "    #             local_grads = self.calculate_gradients(model)\n",
        "    #             r = calculate_r(local_grads, p)\n",
        "\n",
        "    #             # Perform the first step of SAM optimizer\n",
        "    #             optimizer.first_step(r=r, zero_grad=True)\n",
        "\n",
        "    #             # Perform a full forward-backward pass with closure\n",
        "    #             def closure():\n",
        "    #                 optimizer.zero_grad()\n",
        "    #                 log_probs = model(images)\n",
        "    #                 loss = self.criterion(log_probs, labels)\n",
        "    #                 loss.backward()\n",
        "    #                 return loss\n",
        "\n",
        "    #             optimizer.step(r=r, closure=closure)\n",
        "\n",
        "    #             if self.args.verbose and (batch_idx % 10 == 0):\n",
        "    #                 print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "    #                 global_round, iter, batch_idx * len(images),\n",
        "    #                 len(self.trainloader.dataset),\n",
        "    #                 100. * batch_idx / len(self.trainloader), loss.item()))\n",
        "\n",
        "    #             self.logger.add_scalar('loss', loss.item())\n",
        "\n",
        "    #     return model.state_dict()\n",
        "\n",
        "\n",
        "    def update_weights(self, model, global_round, p):\n",
        "        model.train()\n",
        "        optimizer = SAM(model.parameters(), base_optimizer=torch.optim.SGD, rho=0.01, lr=self.args.lr, momentum=0.5)\n",
        "\n",
        "        # Calculate local_grads and r outside the loop\n",
        "        local_grads = self.calculate_gradients(model)\n",
        "        r_values = calculate_r(local_grads, p)\n",
        "\n",
        "        for iter in range(self.args.local_ep):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                # Forward pass and calculate loss\n",
        "                log_probs = model(images)\n",
        "                loss = self.criterion(log_probs, labels)\n",
        "                loss.backward()  # Backpropagate and calculate gradients\n",
        "\n",
        "\n",
        "                # Perform the first step of SAM optimizer\n",
        "                #optimizer.first_step(r=r_values, zero_grad=True)\n",
        "\n",
        "                # Perform a full forward-backward pass with closure\n",
        "                def closure():\n",
        "                    optimizer.zero_grad()\n",
        "                    log_probs = model(images)\n",
        "                    loss = self.criterion(log_probs, labels)\n",
        "                    loss.backward()\n",
        "                    return loss\n",
        "\n",
        "                optimizer.step(r=r_values, closure=closure)\n",
        "\n",
        "                if self.args.verbose and (batch_idx % 10 == 0):\n",
        "                    print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    global_round, iter, batch_idx * len(images),\n",
        "                    len(self.trainloader.dataset),\n",
        "                    100. * batch_idx / len(self.trainloader), loss.item()))\n",
        "\n",
        "                self.logger.add_scalar('loss', loss.item())\n",
        "\n",
        "        return model.state_dict()\n",
        "\n",
        "\n",
        "\n",
        "    def inference(self, model):\n",
        "        model.eval()\n",
        "        loss, total, correct = 0.0, 0.0, 0.0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(self.testloader):\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "            # Inference\n",
        "            outputs = model(images)\n",
        "            batch_loss = self.criterion(outputs, labels)\n",
        "            loss += batch_loss.item()\n",
        "\n",
        "            # Prediction\n",
        "            _, pred_labels = torch.max(outputs, 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "        accuracy = correct/total\n",
        "        return accuracy, loss\n",
        "\n",
        "\n",
        "def test_inference(args, model, test_dataset):\n",
        "    \"\"\" Returns the test accuracy and loss \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    loss, total, correct = 0.0, 0.0, 0.0\n",
        "\n",
        "    # device = 'cuda' if args.gpu else 'cpu'\n",
        "    device = 'cpu'\n",
        "    criterion = nn.NLLLoss().to(device)\n",
        "    testloader = DataLoader(test_dataset, batch_size=128,\n",
        "                            shuffle=False)\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(testloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Inference\n",
        "        outputs = model(images)\n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        loss += batch_loss.item()\n",
        "\n",
        "        # Prediction\n",
        "        _, pred_labels = torch.max(outputs, 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "\n",
        "    accuracy = correct/total\n",
        "    return accuracy, loss\n",
        "\n",
        "\n",
        "\n",
        "class calculate(object):\n",
        "    def __init__(self, args, dataset, idxs, logger, p):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "        self.trainloader, self.validloader, self.testloader, self.sample_size = self.train_val_test(\n",
        "            dataset, list(idxs))\n",
        "        # self.device = 'cuda' if args.gpu else 'cpu'\n",
        "        self.device = 'cpu'\n",
        "        # Default criterion set to NLL loss function\n",
        "        self.criterion = nn.NLLLoss().to(self.device)\n",
        "\n",
        "    def train_val_test(self, dataset, idxs):\n",
        "        \"\"\"\n",
        "        Returns train, validation and test dataloaders for a given dataset\n",
        "        and user indexes.\n",
        "        \"\"\"\n",
        "        # split indexes for train, validation, and test (80, 10, 10)\n",
        "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
        "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
        "        idxs_test = idxs[int(0.9*len(idxs)):]\n",
        "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
        "                                 batch_size=self.args.local_bs, shuffle=True)\n",
        "        validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
        "                                 batch_size=self.args.local_bs, shuffle=False)\n",
        "        testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
        "                                 batch_size=self.args.local_bs, shuffle=False)\n",
        "\n",
        "        # validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
        "        #                          batch_size=int(len(idxs_val)/10), shuffle=False)\n",
        "        # testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
        "        #                         batch_size=int(len(idxs_test)/10), shuffle=False)\n",
        "        sample_size = len(trainloader.dataset)\n",
        "        return trainloader, validloader, testloader, sample_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USjf0D7bbE8A",
        "outputId": "4b77cdbc-ba50-4f9a-b657-b8db2e486967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"...\")\n",
        "\n",
        "def get_dataset(args):\n",
        "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
        "    the keys are the user index and the values are the corresponding data for\n",
        "    each of those users.\n",
        "    \"\"\"\n",
        "\n",
        "    if args.dataset == 'cifar':\n",
        "        data_dir = '../data/cifar/'\n",
        "        train_transform = transforms.Compose(\n",
        "            [transforms.RandomHorizontalFlip(),\n",
        "             transforms.RandomCrop(32, 4),\n",
        "             transforms.ToTensor(),\n",
        "             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] )])\n",
        "\n",
        "        test_transform =transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] )])\n",
        "\n",
        "        train_dataset = datasets.CIFAR10(data_dir, train=True, download=True,\n",
        "                                       transform=train_transform)\n",
        "\n",
        "        test_dataset = datasets.CIFAR10(data_dir, train=False, download=True,\n",
        "                                      transform=test_transform)\n",
        "\n",
        "        # sample training data amongst users\n",
        "        if args.iid:\n",
        "            # Sample IID user data from CIFAR\n",
        "            user_groups = cifar_iid(train_dataset, args.num_users)\n",
        "        else:\n",
        "            # Sample Non-IID user data from CIFAR\n",
        "            if args.unequal:\n",
        "                # Chose uneuqal splits for every user\n",
        "                raise NotImplementedError()\n",
        "            else:\n",
        "                # Chose euqal splits for every user\n",
        "                user_groups = cifar_noniid(train_dataset, args.num_users)\n",
        "\n",
        "    elif args.dataset == 'mnist' or 'fmnist':\n",
        "        if args.dataset == 'mnist':\n",
        "            data_dir = '../data/mnist/'\n",
        "        else:\n",
        "            data_dir = '../data/fmnist/'\n",
        "\n",
        "        apply_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "        train_dataset = datasets.MNIST(data_dir, train=True, download=True,\n",
        "                                       transform=apply_transform)\n",
        "\n",
        "        test_dataset = datasets.MNIST(data_dir, train=False, download=True,\n",
        "                                      transform=apply_transform)\n",
        "\n",
        "        # sample training data amongst users\n",
        "        if args.iid:\n",
        "            # Sample IID user data from Mnist\n",
        "            user_groups = mnist_iid(train_dataset, args.num_users)\n",
        "        else:\n",
        "            # Sample Non-IID user data from Mnist\n",
        "            if args.unequal:\n",
        "                # Chose uneuqal splits for every user\n",
        "                user_groups = mnist_noniid_unequal(train_dataset, args.num_users)\n",
        "            else:\n",
        "                # Chose euqal splits for every user\n",
        "                user_groups = mnist_noniid(train_dataset, args.num_users)\n",
        "\n",
        "    return train_dataset, test_dataset, user_groups\n",
        "\n",
        "\n",
        "# def calculate_r(local_grads, p):\n",
        "#     len_grads = len(local_grads)\n",
        "#     r_k = np.zeros(len_grads)\n",
        "\n",
        "#     for i in range(len_grads):\n",
        "#         flattened_i = local_grads[i].flatten()  # Flatten the gradient tensor\n",
        "#         cos_sims = [torch.cosine_similarity(flattened_i, local_grads[j].flatten(), dim=0) for j in range(len_grads)]\n",
        "#         r_k[i] = sum(p[j] * cos_sims[j] for j in range(len_grads))\n",
        "#         r_k[i] /= np.linalg.norm(flattened_i)\n",
        "\n",
        "#     return r_k\n",
        "\n",
        "\n",
        "# def calculate_r(local_grads, weight_coefficients):\n",
        "#     num_clients = len(local_grads)\n",
        "#     r_k = np.zeros(num_clients)\n",
        "#     #weight_coefficients_tensor = [torch.tensor(p) for p in weight_coefficients]\n",
        "#     for i in range(num_clients):\n",
        "#         dot_product = sum(weight_coefficients[j] * torch.sum(local_grads[i] * local_grads[j]) for j in range(num_clients))\n",
        "\n",
        "#         norm_i = torch.norm(local_grads[i]).item()\n",
        "#         r_k[i] = dot_product / (norm_i * norm_i)\n",
        "\n",
        "#     return r_k\n",
        "\n",
        "\n",
        "# def calculate_r(local_grads, weight_coefficients):\n",
        "#     num_clients = len(local_grads)\n",
        "#     r_k = np.zeros(num_clients)\n",
        "#     weight_coefficients_tensor = [torch.tensor(p) for p in weight_coefficients]\n",
        "#     for i in range(num_clients):\n",
        "#       flat_i = torch.cat([p.view(-1) for p in local_grads[i]])\n",
        "#       dot_product = sum(weight_coefficients_tensor[j] * torch.sum(flat_i * flat_i) for j in range(num_clients))\n",
        "#       sum_squared_i = torch.sum(flat_i * flat_i)\n",
        "#       r_k[i] = dot_product / (sum_squared_i * sum_squared_i)\n",
        "\n",
        "\n",
        "#     return r_k\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_r(local_grads, weight_coefficients):\n",
        "#     num_clients = len(local_grads)\n",
        "#     r_k = np.zeros(num_clients)\n",
        "#     weight_coefficients_tensor = [torch.tensor(p) for p in weight_coefficients]\n",
        "#     for i in range(num_clients):\n",
        "#       flat_i = torch.cat([p.view(-1) for p in local_grads[i]])\n",
        "#       dot_product = sum(weight_coefficients_tensor[j] * torch.sum(flat_i * flat_i) for j in range(num_clients))\n",
        "#       sum_squared_i = torch.sum(flat_i * flat_i)\n",
        "#       r_k[i] = dot_product / (sum_squared_i)\n",
        "\n",
        "\n",
        "#     return r_k\n",
        "\n",
        "def calculate_r(local_grads, weight_coefficients):\n",
        "    num_clients = len(local_grads)\n",
        "    r_k = np.zeros(num_clients)\n",
        "    weight_coefficients_tensor = [torch.tensor(p) for p in weight_coefficients]\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        flat_i = torch.cat([p.view(-1) for p in local_grads[i]])\n",
        "\n",
        "        # Initialize dot_product for client i\n",
        "        dot_product = 0.0\n",
        "\n",
        "        for j in range(num_clients):\n",
        "            flat_j = torch.cat([p.view(-1) for p in local_grads[j]])\n",
        "\n",
        "            # Perform element-wise multiplication only if the sizes match\n",
        "            if flat_i.size() == flat_j.size():\n",
        "                dot_product += weight_coefficients_tensor[j] * torch.sum(flat_i * flat_j)\n",
        "\n",
        "        # Calculate the sum of squared values in the gradient of client i\n",
        "        sum_squared_i = torch.sum(flat_i * flat_i)\n",
        "\n",
        "        # Calculate the radius for client i\n",
        "        r_k[i] = dot_product / sum_squared_i\n",
        "\n",
        "    return r_k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def calculate_r(local_grads, p):\n",
        "#     len_grads = len(local_grads)\n",
        "#     r_k = np.zeros(len_grads)\n",
        "\n",
        "\n",
        "#     for i in range(len_grads):\n",
        "#         r_k[i] = sum(p[j] * np.dot(local_grads[i], local_grads[j]) for j in range(len_grads))\n",
        "#         r_k[i] = r_k[i] / (np.linalg.norm(local_grads[i]) * np.linalg.norm(local_grads[i]))\n",
        "\n",
        "#     return r_k\n",
        "\n",
        "\n",
        "def average_weights(local_weights):\n",
        "    #here we are returning the average of the weights at the\n",
        "    #level of the server. Thus, we don't need to change it here\n",
        "    \"\"\"\n",
        "    Returns the average of the weights.\n",
        "    \"\"\"\n",
        "    w_avg = copy.deepcopy(local_weights[0])\n",
        "    for key in w_avg.keys():\n",
        "        for i in range(1, len(local_weights)):\n",
        "            w_avg[key] += local_weights[i][key]\n",
        "        w_avg[key] = torch.div(w_avg[key], len(local_weights))\n",
        "    return w_avg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exp_details(args):\n",
        "    print('\\nExperimental details:')\n",
        "    print(f'    Model     : {args.model}')\n",
        "    print(f'    Dataset   : {args.dataset}')\n",
        "    print(f'    Optimizer : {args.optimizer}')\n",
        "    print(f'    Learning  : {args.lr}')\n",
        "    print(f'    Global Rounds   : {args.epochs}\\n')\n",
        "\n",
        "    print('    Federated parameters:')\n",
        "    if args.iid:\n",
        "        print('    IID')\n",
        "    else:\n",
        "        print('    Non-IID')\n",
        "    print(f'    Fraction of users  : {args.frac}')\n",
        "    print(f'    Local Batch size   : {args.local_bs}')\n",
        "    print(f'    Local Epochs       : {args.local_ep}\\n')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sktIHum-bG0K",
        "outputId": "2f3df053-7c01-4735-b42b-b919242e18be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new copy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print(\"new copy\")\n",
        "\n",
        "\n",
        "def mnist_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    mnist_iid takes in a dataset and the number of users as input and returns a dictionary that represents\n",
        "    the division of the datasets among the users in an approximately equal and independent manner\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users) #the number of items each user will get\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    #the dictionary will store the division of datasets among users\n",
        "    #all_idxs will store the indices of the datasets\n",
        "\n",
        "\n",
        "    for i in range(num_users): #looping over all users\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        #randomly selecting \"num_items\" from all_idxs without replacement, and turn them into a set {...}\n",
        "\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "        #after assigning itms to user, this line updates \"all_idxs\" to remove the indices  that have already been selected\n",
        "\n",
        "    return dict_users #the division od the dataset among users {set}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mnist_noniid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    divide mnist dataset aong multiple users in a non-iid manner. This division ensures that each user has\n",
        "    a unique and potentially different subset of data\n",
        "    \"\"\"\n",
        "    # 60,000 training imgs -->  200 imgs/shard X 300 shards\n",
        "    #variable initialization\n",
        "    num_shards, num_imgs = 200, 300\n",
        "    idx_shard = [i for i in range(num_shards)] #this list is used to to keep track of available shards during assignment process\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)} #to store the non-iid division of dataset among users\n",
        "\n",
        "    #i:np.array([]) creating a key depending on i and value is the empty arrays: example: {1:[], 2:[], 3:[] ....}\n",
        "\n",
        "    idxs = np.arange(num_shards*num_imgs) #used to assign images to users\n",
        "    labels = dataset.train_labels.numpy()\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels)) #vertically stacking idxs and labels. it creates a matrix of 2 rows, 1st = idxs, 2nd = labels\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()] #sort labels based on the second row in ascending order\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "\n",
        "    # divide and assign 2 shards/client\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, 2, replace=False)) #choose 2 from \"200\" without replacement\n",
        "        #{randomly selecting 2 shards without replacement and store them in rand_set}\n",
        "\n",
        "        idx_shard = list(set(idx_shard) - rand_set) #remove the selected ones to ensure that the same shards aren't selected again\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "            #appending the empty arrays with this interval\n",
        "\n",
        "    return dict_users  #non-iid division of the dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mnist_noniid_unequal(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from MNIST dataset where clients have an unequal amount of data\n",
        "    \"\"\"\n",
        "\n",
        "    # 60,000 training imgs --> 50 imgs/shard X 1200 shards\n",
        "    num_shards, num_imgs = 1200, 50  #1200 = number of shards into which dataset is divided. 50 = images in each dataset\n",
        "    idx_shard = [i for i in range(num_shards)] #this list will keep track of available shards during assignment process\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)} #dict that will store the non-iid division of dataset among users\n",
        "    idxs = np.arange(num_shards*num_imgs) #creating a sequence of indices which will be used o assign images to users\n",
        "    labels = dataset.train_labels.numpy()\n",
        "\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels)) #vertically stacking idxs and labels, to get a matrix of 2 rows.\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()] #sort based on the second row = labels\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # Minimum and maximum shards assigned per client:\n",
        "    min_shard = 1\n",
        "    max_shard = 30\n",
        "\n",
        "    # Divide the shards into random chunks for every client such that the sum of these chunks = num_shards\n",
        "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
        "                                          size=num_users)\n",
        "    random_shard_size = np.around(random_shard_size /\n",
        "                                  sum(random_shard_size) * num_shards)\n",
        "    random_shard_size = random_shard_size.astype(int)\n",
        "\n",
        "    # the following parts handles the assignment of shards to users\n",
        "\n",
        "    if sum(random_shard_size) > num_shards:\n",
        "        #if the random shard size is more than the total number of available shards, each user is assigned one shard first\n",
        "        #to ensure they have at least one, and the rest are distributed randomly based on the \"random_shared_size\"\n",
        "        for i in range(num_users):\n",
        "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "        random_shard_size = random_shard_size-1\n",
        "\n",
        "        # Next, randomly assign the remaining shards\n",
        "        for i in range(num_users):\n",
        "            if len(idx_shard) == 0:\n",
        "                continue\n",
        "            shard_size = random_shard_size[i]\n",
        "            if shard_size > len(idx_shard):\n",
        "                shard_size = len(idx_shard)\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "    else:\n",
        "        # if random_shard_size is less than or equal to the total number of available shards, then shards are directly assigned to users\n",
        "        #based on the calculated \"random_shard_size\". Any remaining shard are then assigned to the user with minimum number of images\n",
        "        for i in range(num_users):\n",
        "            shard_size = random_shard_size[i]\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "        if len(idx_shard) > 0:\n",
        "            # Add the leftover shards to the client with minimum images:\n",
        "            shard_size = len(idx_shard)\n",
        "            # Add the remaining shard to the client with lowest data\n",
        "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[k] = np.concatenate(\n",
        "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def cifar_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    creates iid data distribution among clients for CIFAR-10 dataset\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users) #data points per each user\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))] #dictionary to store the indicies and array to store the indices of the dataset\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        #randomly selects \"num_items\" from\"all_idxs\" without replacement\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i]) #remove the idxs that have been already selected\n",
        "\n",
        "\n",
        "    return dict_users #dictionary representing the IID data distribution among clients\n",
        "\n",
        "\n",
        "def cifar_noniid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    creates non-iid data distribution among clients for the CIFAR-10 dataset\n",
        "    divides dataset into subsets and assign them to each user, ensuring a varying distribution of data\n",
        "    \"\"\"\n",
        "    num_shards, num_imgs = 200, 250\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    labels = dataset.train_labels.numpy()\n",
        "    #labels = np.array(train_dataset)\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # divide and assign\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, 2, replace=False)) #randomly selecting 2 shards from the available shards (idxs_shards); without replacement\n",
        "        idx_shard = list(set(idx_shard) - rand_set)\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an5ZFTHcbN52"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the SAM optimizer\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    #define the SAM optimizer class which inherits from torch.optim.Optimizer\n",
        "    def __init__(self, params, base_optimizer, rho=0.03, **kwargs):\n",
        "        #assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs) #dictionary that holds default values for rho, adaptive, and any aditional keyword arguments\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups #param_groups: attribute of SAM, is set to match the parameter groups of the base-optimizer\n",
        "        self.defaults.update(self.base_optimizer.defaults) #same as the defaults\n",
        "\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    # # This method modifies the parameters based on the SAM technique. It performs a forward pass with gradient computation but does not update the actual parameters.\n",
        "    # def first_step(self,  r, zero_grad=False):\n",
        "    #     grad_norm = self._grad_norm()\n",
        "    #     for group in self.param_groups: #for each parameter group\n",
        "    #         scale = r * group[\"rho\"] / (grad_norm + 1e-12)\n",
        "    #         #   scale = self.rho / (2 * grad_norm + 1e-12)  # Updated scale calculation\n",
        "    #         for p in group[\"params\"]: #For each parameter in the group, its data is being cloned and stored\n",
        "    #             if p.grad is None: continue\n",
        "\n",
        "\n",
        "\n",
        "    #             self.state[p][\"old_p\"] = p.data.clone()\n",
        "    #             print(\"p.grad shape:\", p.grad.shape)\n",
        "    #             print(\"scale shape:\", scale.shape)\n",
        "    #             #e_w = p.grad * scale.to(p).unsqueeze(1)\n",
        "    #             e_w = p.grad * scale.view(1, -1).repeat(p.grad.size(0), 1).to(p)\n",
        "    #             p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "    #     if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    # This method modifies the parameters based on the SAM technique. It performs a forward pass with gradient computation but does not update the actual parameters.\n",
        "    def first_step(self,  r, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups: #for each parameter group\n",
        "            for p in group[\"params\"]:\n",
        "              if p.grad is None:\n",
        "                continue\n",
        "              self.state[p][\"old_p\"] = p.data.clone()\n",
        "              for r_k, grad_elem in zip(r, p.grad):\n",
        "                scale = r_k * group[\"rho\"] / (grad_norm + 1e-12)\n",
        "                e_w = grad_elem * scale.to(p)\n",
        "                p.add_(e_w)\n",
        "\n",
        "        if zero_grad:\n",
        "          self.zero_grad()\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    #This method undoes the parameter modifications from first_step and performs the actual optimization step.\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    #This method performs a full optimization step using the SAM technique. It requires a closure, which is a function containing the forward and backward passes.\n",
        "    def step(self,r,  closure):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(r, zero_grad=True) #The first_step is performed with zeroed gradients.\n",
        "        closure() #The closure is executed (forward-backward pass).\n",
        "        self.second_step() #The second_step is executed to perform the actual optimization update.\n",
        "\n",
        "\n",
        "\n",
        "    def _grad_norm(self):\n",
        "      shared_device = self.param_groups[0][\"params\"][0].device\n",
        "      grad_tensors = [\n",
        "        p.grad.norm(p=2).to(shared_device)\n",
        "        for group in self.param_groups for p in group[\"params\"]\n",
        "        if p.grad is not None\n",
        "      ]\n",
        "      if len(grad_tensors) > 0:\n",
        "        norm = torch.norm(torch.stack(grad_tensors), p=2)\n",
        "      else:\n",
        "        norm = torch.tensor(1e-12).to(shared_device)\n",
        "      return norm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        #this method loads the state dictionary of the optimizer.\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7kfrryvbSax",
        "outputId": "37448274-2048-44aa-b57a-e4dffe71f0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experimental details:\n",
            "    Model     : resnet\n",
            "    Dataset   : cifar\n",
            "    Optimizer : adam\n",
            "    Learning  : 0.1\n",
            "    Global Rounds   : 10\n",
            "\n",
            "    Federated parameters:\n",
            "    IID\n",
            "    Fraction of users  : 0.1\n",
            "    Local Batch size   : 128\n",
            "    Local Epochs       : 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "start_time = time.time() #sets the start time to current time\n",
        "\n",
        "#define paths\n",
        "path_project= os.path.abspath('..')\n",
        "logger = SummaryWriter('../logs')\n",
        "\n",
        "args = args_parser()\n",
        "exp_details(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfTGzB0lbdfh",
        "outputId": "3e37d2e5-5dbc-463a-aedf-14d879855e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170498071/170498071 [00:01<00:00, 102482604.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/cifar/cifar-10-python.tar.gz to ../data/cifar/\n",
            "Files already downloaded and verified\n",
            "global model\n",
            "ResNet18(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = 'cpu'\n",
        "\n",
        "# load dataset and user groups\n",
        "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
        "\n",
        "# BUILD MODEL\n",
        "\n",
        "if args.model == 'cnn':\n",
        "  if args.dataset == 'mnist':\n",
        "    global_model = CNNMnist(args=args)\n",
        "  elif args.dataset == 'fmnist':\n",
        "    global_model = CNNFashion_Mnist(args=args)\n",
        "  elif args.dataset == 'cifar':\n",
        "    global_model = CNNCifar(args=args)\n",
        "\n",
        "\n",
        "elif args.model == 'mlp':\n",
        "  img_size = train_dataset[0][0].shape\n",
        "  len_in = 1\n",
        "  for x in img_size:\n",
        "    len_in *= x\n",
        "    global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
        "                               dim_out=args.num_classes)\n",
        "elif args.model == 'resnet':\n",
        "  global_model = ResNet18(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "\n",
        "else:\n",
        "  exit('Error: unrecognized model')\n",
        "\n",
        "print(\"global model\")\n",
        "print(global_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NItA7kKebhAL",
        "outputId": "31996510-1da8-4cda-8f72-1407974cab50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Move the global model to the specified device\n",
        "global_model.to(device)\n",
        "\n",
        "# Set the global model to training mode\n",
        "global_model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnYEencqbkRD"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "train_loss, train_accuracy = [], []\n",
        "val_acc_list, net_list = [], []\n",
        "cv_loss, cv_acc = [], []\n",
        "\n",
        "print_every = 2 #specifies how often to print the training progress during training.\n",
        "\n",
        "#variables to check if the validation loss has stopped improving i.e. overfitting\n",
        "val_loss_pre, counter = 0, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeBhA8P1bmuo",
        "outputId": "632dc22a-8560-4b70-cefc-6aeaa9b2d93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n"
          ]
        }
      ],
      "source": [
        "# Randomly shuffle the indices of users\n",
        "users_pool = np.random.choice(range(args.num_users), args.num_users, replace=False)\n",
        "users_pool.sort()\n",
        "\n",
        "# Calculate weight coefficients for each client\n",
        "weight_coefficient_p = []\n",
        "for idx in users_pool:\n",
        "    local_model = calculate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, p=0)\n",
        "    a, b, d, c = local_model.train_val_test(dataset=train_dataset, idxs=list(user_groups[idx]))\n",
        "    weight_coefficient_p.append(c)\n",
        "\n",
        "# Normalize weight coefficients\n",
        "total_size = sum(weight_coefficient_p)\n",
        "weight_coefficient_p = [number / total_size for number in weight_coefficient_p]\n",
        "print(weight_coefficient_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg0zvBQ3bopk",
        "outputId": "897a67a1-0ca2-4411-9649-fac4584a2dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.099\n",
            "\n",
            " | Global Training Round : 1 |\n",
            "\n",
            "[20 14  4 84 76 30 19 25 64 88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-63fd145f8034>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(image), torch.tensor(label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.057535   0.05765288 0.05674439 0.05348188 0.05342261 0.05760142\n",
            " 0.05490991 0.05355322 0.0570736  0.0511042 ]\n",
            "[0.0057535  0.00576529 0.00567444 0.00534819 0.00534226 0.00576014\n",
            " 0.00549099 0.00535532 0.00570736 0.00511042]\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -344.839813\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -590.444031\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -344.480499\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -591.120300\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -342.639648\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -590.782288\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -348.801971\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -571.577087\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -323.246857\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -572.090698\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -350.971375\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -595.624939\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -352.413818\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -602.958435\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -339.290436\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -595.212952\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -346.561279\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -581.684692\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -348.176666\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -580.871887\n",
            "Global Weights:\n",
            "0.09801\n",
            "\n",
            " | Global Training Round : 2 |\n",
            "\n",
            "[27 56 73 17 75 49 63  6 29 61]\n",
            "[0.07590308 0.08000397 0.09026952 0.08296521 0.07413893 0.07313354\n",
            " 0.08159728 0.08597334 0.08060515 0.07291178]\n",
            "[0.00759031 0.0080004  0.00902695 0.00829652 0.00741389 0.00731335\n",
            " 0.00815973 0.00859733 0.00806051 0.00729118]\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2228.592773\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3844.424561\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2245.895264\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3811.279785\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2229.143799\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3632.666748\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2223.673096\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3808.102783\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2193.025391\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3748.276611\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2222.935303\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3767.275635\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2217.981934\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3738.292725\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2214.492676\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3749.627441\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2229.542480\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3684.616455\n",
            "| Global Round : 1 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2202.057129\n",
            "| Global Round : 1 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -3782.011963\n",
            "Global Weights:\n",
            " \n",
            "Avg Training Stats after 2 global rounds:\n",
            "Training Loss : nan\n",
            "Train Accuracy: 21.08% \n",
            "\n",
            "0.0970299\n",
            "\n",
            " | Global Training Round : 3 |\n",
            "\n",
            "[45 22 71 72 81 55 99 27 87 97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08493072 0.07944954 0.08223518 0.06872048 0.07049341 0.05869118\n",
            " 0.08668152 0.09053168 0.07446194 0.08281495]\n",
            "[0.00849307 0.00794495 0.00822352 0.00687205 0.00704934 0.00586912\n",
            " 0.00866815 0.00905317 0.00744619 0.00828149]\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10077.505859\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17939.628906\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10084.332031\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17503.718750\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10186.186523\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17583.474609\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10215.953125\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -18039.408203\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -9988.715820\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -18108.349609\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -9639.311523\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17749.925781\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10204.069336\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -18060.714844\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10113.188477\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17946.695312\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -10156.705078\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17281.699219\n",
            "| Global Round : 2 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -9978.220703\n",
            "| Global Round : 2 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -17597.210938\n",
            "Global Weights:\n",
            "0.096059601\n",
            "\n",
            " | Global Training Round : 4 |\n",
            "\n",
            "[20 23 15 69 90 31 35 11 81 29]\n",
            "[0.0557272  0.07508065 0.0875722  0.0730506  0.08162227 0.08840208\n",
            " 0.08797956 0.06495033 0.08473684 0.05930161]\n",
            "[0.00557272 0.00750806 0.00875722 0.00730506 0.00816223 0.00884021\n",
            " 0.00879796 0.00649503 0.00847368 0.00593016]\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41739.253906\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -73492.085938\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41955.902344\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -74563.179688\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -42016.246094\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -75159.476562\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41689.093750\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -73524.007812\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41612.976562\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -74131.804688\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41213.832031\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -74973.085938\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -40465.957031\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -73651.718750\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41103.726562\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -74314.632812\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -42297.257812\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -71828.187500\n",
            "| Global Round : 3 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -41553.476562\n",
            "| Global Round : 3 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -73547.421875\n",
            "Global Weights:\n",
            " \n",
            "Avg Training Stats after 4 global rounds:\n",
            "Training Loss : nan\n",
            "Train Accuracy: 14.00% \n",
            "\n",
            "0.09509900499\n",
            "\n",
            " | Global Training Round : 5 |\n",
            "\n",
            "[ 0 18 46 38 76 66 50  9 94 57]\n",
            "[0.09512553 0.0779391  0.08633877 0.07858998 0.02931198 0.08977985\n",
            " 0.09465256 0.07046313 0.09157463 0.02228688]\n",
            "[0.00951255 0.00779391 0.00863388 0.007859   0.0029312  0.00897799\n",
            " 0.00946526 0.00704631 0.00915746 0.00222869]\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -160956.093750\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -286158.500000\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -162977.953125\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -288288.750000\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -162858.828125\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -289933.750000\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -161265.859375\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -287333.531250\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -163005.593750\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -283260.500000\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -163166.078125\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -289502.250000\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -160224.125000\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -288648.656250\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -163084.671875\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -293557.656250\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -163664.046875\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -284501.343750\n",
            "| Global Round : 4 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -161089.156250\n",
            "| Global Round : 4 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -285233.781250\n",
            "Global Weights:\n",
            "0.0941480149401\n",
            "\n",
            " | Global Training Round : 6 |\n",
            "\n",
            "[63 70 83 44 39 16  1 92 36 48]\n",
            "[0.04962466 0.06569324 0.07624505 0.05701764 0.06696426 0.05539566\n",
            " 0.08368889 0.09028194 0.0911798  0.07293752]\n",
            "[0.00496247 0.00656932 0.00762451 0.00570176 0.00669643 0.00553957\n",
            " 0.00836889 0.00902819 0.00911798 0.00729375]\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -607247.687500\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1091890.875000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -608876.125000\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1086476.500000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -613227.062500\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1078748.875000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -607633.562500\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1081155.125000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -611756.750000\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1085431.375000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -610566.812500\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1085918.750000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -614833.875000\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1098487.875000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -606107.875000\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1081617.250000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -616109.000000\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1072963.250000\n",
            "| Global Round : 5 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -615080.500000\n",
            "| Global Round : 5 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -1091123.000000\n",
            "Global Weights:\n",
            " \n",
            "Avg Training Stats after 6 global rounds:\n",
            "Training Loss : nan\n",
            "Train Accuracy: 10.00% \n",
            "\n",
            "0.093206534790699\n",
            "\n",
            " | Global Training Round : 7 |\n",
            "\n",
            "[71 49 87 65 15 21 40 48 58 56]\n",
            "[0.077448   0.0791912  0.04060887 0.08169135 0.07230969 0.09234156\n",
            " 0.08797497 0.08482458 0.08511798 0.08901659]\n",
            "[0.0077448  0.00791912 0.00406089 0.00816914 0.00723097 0.00923416\n",
            " 0.0087975  0.00848246 0.0085118  0.00890166]\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2253403.500000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4059697.500000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2262744.500000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4044343.250000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2262050.000000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4083294.250000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2265897.000000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4041118.500000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2259186.250000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4018512.750000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2261712.750000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4026541.750000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2273968.000000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4026487.750000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2264736.500000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4007116.000000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2276429.000000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4033333.000000\n",
            "| Global Round : 6 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -2247819.250000\n",
            "| Global Round : 6 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -4025246.500000\n",
            "Global Weights:\n",
            "0.09227446944279201\n",
            "\n",
            " | Global Training Round : 8 |\n",
            "\n",
            "[44  6  0  9 50 78 31 61 71 38]\n",
            "[0.06040898 0.09909283 0.05380282 0.04990363 0.03333492 0.04981089\n",
            " 0.08246317 0.08358158 0.08305465 0.08324222]\n",
            "[0.0060409  0.00990928 0.00538028 0.00499036 0.00333349 0.00498109\n",
            " 0.00824632 0.00835816 0.00830546 0.00832422]\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8291849.500000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14936070.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8280155.000000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14805235.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8285226.500000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14729756.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8313548.500000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14886879.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8267298.500000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14690030.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8245069.500000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14746615.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8325364.000000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14724947.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8295030.000000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14829724.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8293338.000000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14795958.000000\n",
            "| Global Round : 7 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -8253862.500000\n",
            "| Global Round : 7 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -14895920.000000\n",
            "Global Weights:\n",
            " \n",
            "Avg Training Stats after 8 global rounds:\n",
            "Training Loss : nan\n",
            "Train Accuracy: 6.00% \n",
            "\n",
            "0.09135172474836409\n",
            "\n",
            " | Global Training Round : 9 |\n",
            "\n",
            "[32  4 51 75  2 69 84  0 22 29]\n",
            "[0.06101322 0.04989538 0.05193578 0.01970499 0.09395585 0.07774755\n",
            " 0.05251601 0.09261344 0.0822737  0.09652828]\n",
            "[0.00610132 0.00498954 0.00519358 0.0019705  0.00939559 0.00777476\n",
            " 0.0052516  0.00926134 0.00822737 0.00965283]\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -29832226.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53668084.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -29977584.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53027732.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -30044044.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53011296.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -30168184.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -52963204.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -29975368.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53247240.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -30186460.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -52824056.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -29947972.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53389832.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -29864174.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53203968.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -30161290.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53440764.000000\n",
            "| Global Round : 8 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -30120672.000000\n",
            "| Global Round : 8 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -53875744.000000\n",
            "Global Weights:\n",
            "0.09043820750088044\n",
            "\n",
            " | Global Training Round : 10 |\n",
            "\n",
            "[53 20 13 81 24 61 17 52 99 70]\n",
            "[0.09799599 0.09225745 0.09528804 0.0966005  0.09780073 0.09681626\n",
            " 0.09702086 0.09714165 0.09636939 0.09172332]\n",
            "[0.0097996  0.00922574 0.0095288  0.00966005 0.00978007 0.00968163\n",
            " 0.00970209 0.00971417 0.00963694 0.00917233]\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -107828176.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -190991520.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -108098120.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -190993360.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -107673936.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -191927200.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -107287248.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -191100800.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -107399936.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -191105824.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -108634280.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -192880528.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -108260120.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -192349952.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -108316280.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -193108576.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -107202232.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -189616896.000000\n",
            "| Global Round : 9 | Local Epoch : 0 | [0/400 (0%)]\tLoss: -108005832.000000\n",
            "| Global Round : 9 | Local Epoch : 1 | [0/400 (0%)]\tLoss: -190062320.000000\n",
            "Global Weights:\n",
            " \n",
            "Avg Training Stats after 10 global rounds:\n",
            "Training Loss : nan\n",
            "Train Accuracy: 6.00% \n",
            "\n",
            " \n",
            " Results after 10 global rounds of training:\n",
            "|---- Avg Train Accuracy: 6.00%\n",
            "|---- Test Accuracy: 10.00%\n",
            "\n",
            " Total Run Time: 4179.6941\n"
          ]
        }
      ],
      "source": [
        "# for epoch in tqdm(range(args.epochs)):\n",
        "for epoch in range(args.epochs):\n",
        "    args.lr *= 0.99\n",
        "    #print(\"the new learning rate\")\n",
        "    print(args.lr)\n",
        "    loss_pool = []\n",
        "    local_losses =  []  # List to store local models' weights\n",
        "    local_gradients = []\n",
        "    local_weights_list = []  # Create a list to store local weights\n",
        "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
        "\n",
        "    global_model.train()\n",
        "    # Select a subset of users\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "    print(idxs_users)\n",
        "\n",
        "    for idx in idxs_users:\n",
        "        local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, r=1)\n",
        "        #print(\"awesome so far\")\n",
        "\n",
        "        # Calculate gradients for one epoch\n",
        "        # local_grads = local_model.calculate_gradients(model=copy.deepcopy(global_model))  #get the average gradient of all the batch!\n",
        "        # # for grad in local_grads:\n",
        "        # #   print(f'Local gradient shape: {grad.shape}')\n",
        "        # local_gradients.extend(local_grads) #append to the list and go out of the loop\n",
        "\n",
        "        local_grads = local_model.calculate_gradients(model=copy.deepcopy(global_model))\n",
        "        local_gradients.append(local_grads)\n",
        "\n",
        "\n",
        "\n",
        "    #calculate r globally, not inside the for loop\n",
        "    r = calculate_r(local_gradients, weight_coefficient_p)\n",
        "    #print(\"r is ready\")\n",
        "    print(r)\n",
        "    r = r * 0.1\n",
        "    print(r)\n",
        "\n",
        "\n",
        "\n",
        "    for idx in idxs_users:\n",
        "       local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, r=r)\n",
        "       local_weights = local_model.update_weights(model = copy.deepcopy(global_model), global_round= epoch , p=weight_coefficient_p)\n",
        "       local_weights_list.append(copy.deepcopy(local_weights))\n",
        "\n",
        "\n",
        "    global_weights = average_weights (local_weights_list)\n",
        "    print(\"Global Weights:\")\n",
        "    global_model.load_state_dict(global_weights)\n",
        "    #print(global_weights)\n",
        "\n",
        "     #global_model.load_state_dict(global_weights)\n",
        "    list_acc, list_loss = [], []\n",
        "    global_model.eval()\n",
        "    for c in range(args.num_users):\n",
        "            local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
        "                                      idxs=user_groups[idx], logger=logger, r=r)\n",
        "            acc, loss = local_model.inference(model=global_model)\n",
        "            list_acc.append(acc)\n",
        "            list_loss.append(loss)\n",
        "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
        "\n",
        "    #print global training loss after every 'i' rounds\n",
        "    if (epoch+1) % print_every == 0:\n",
        "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
        "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
        "            print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
        "\n",
        "#Test inference after completion of training\n",
        "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
        "\n",
        "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
        "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
        "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
        "\n",
        "#     # Saving the objects train_loss and train_accuracy:\n",
        "file_name = 'save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
        "    format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
        "               args.local_ep, args.local_bs)\n",
        "\n",
        "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAHye3atbvEU"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "plt.ion()\n",
        "plt.figure()\n",
        "plt.plot(range(len(train_accuracy)), train_accuracy)\n",
        "plt.title('Training accuracy vs communication rounds')\n",
        "plt.ylabel('Training accuracy')\n",
        "plt.xlabel('Communication round')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"/content/train_accuracy.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5yW7lp8cnpJ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/train_accuracy.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvFYwpmCdqCG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}