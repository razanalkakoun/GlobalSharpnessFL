{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYpLKDLJnr2p",
        "outputId": "8cfdf778-b6fc-43f1-dda8-15d11eaaa472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/101.7 kB\u001b[0m \u001b[31m913.0 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "!pip install tensorboardX\n",
        "import tensorboardX\n",
        "from tensorboardX import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK6GXSjpnqyF"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "def args_parser():\n",
        "\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f')\n",
        "\n",
        "    # federated arguments (Notation for the arguments followed from paper)\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=10,\n",
        "                        help=\"number of rounds of training\")\n",
        "    parser.add_argument('--num_users', type=int, default=100,\n",
        "                        help=\"number of users: K\")\n",
        "    parser.add_argument('--frac', type=float, default=0.1,\n",
        "                        help='the fraction of clients: C')\n",
        "    parser.add_argument('--local_ep', type=int, default=2,\n",
        "                        help=\"the number of local epochs: E\")\n",
        "    parser.add_argument('--local_bs', type=int, default=64,\n",
        "                        help=\"local batch size: B\")\n",
        "    parser.add_argument('--lr', type=float, default=0.1,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--momentum', type=float, default=0.99,\n",
        "                        help='SGD momentum (default: 0.5)')\n",
        "\n",
        "\n",
        "    # model arguments\n",
        "    parser.add_argument('--model', type=str, default='mlp', help='model name')\n",
        "    parser.add_argument('--kernel_num', type=int, default=9,\n",
        "                        help='number of each kind of kernel')\n",
        "    parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
        "                        help='comma-separated kernel size to \\\n",
        "                        use for convolution')\n",
        "    parser.add_argument('--num_channels', type=int, default=1, help=\"number \\\n",
        "                        of channels of imgs\")\n",
        "    parser.add_argument('--norm', type=str, default='batch_norm',\n",
        "                        help=\"batch_norm, layer_norm, or None\")\n",
        "    parser.add_argument('--num_filters', type=int, default=32,\n",
        "                        help=\"number of filters for conv nets -- 32 for \\\n",
        "                        mini-imagenet, 64 for omiglot.\")\n",
        "    parser.add_argument('--max_pool', type=str, default='True',\n",
        "                        help=\"Whether use max pooling rather than \\\n",
        "                        strided convolutions\")\n",
        "\n",
        "\n",
        "    # other arguments\n",
        "    parser.add_argument('--dataset', type=str, default='mnist', help=\"name \\\n",
        "                        of dataset\")\n",
        "    parser.add_argument('--num_classes', type=int, default=10, help=\"number \\\n",
        "                        of classes\")\n",
        "    parser.add_argument('--gpu_id', default=None, help=\"To use cuda, set \\\n",
        "                        to a specific GPU ID. Default set to use CPU.\")\n",
        "    parser.add_argument('--optimizer', type=str, default='adam', help=\"type \\\n",
        "                        of optimizer\")\n",
        "    parser.add_argument('--iid', type=int, default=1,\n",
        "                        help='Default set to IID. Set to 0 for non-IID.')\n",
        "    parser.add_argument('--unequal', type=int, default=0,\n",
        "                        help='whether to use unequal data splits for  \\\n",
        "                        non-i.i.d setting (use 0 for equal splits)')\n",
        "    parser.add_argument('--stopping_rounds', type=int, default=10,\n",
        "                        help='rounds of early stopping')\n",
        "    parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
        "    parser.add_argument('--seed', type=int, default=1, help='random seed')\n",
        "    parser.add_argument('--alpha', type=float, default=0.5, help=\"non-iidness parameter\")\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuZRQpv6hANI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "def mnist_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def mnist_noniid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from MNIST dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 60,000 training imgs -->  200 imgs/shard X 300 shards\n",
        "\n",
        "    num_shards, num_imgs = 200, 300\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    labels = dataset.train_labels.numpy()\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # divide and assign 2 shards/client\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
        "        idx_shard = list(set(idx_shard) - rand_set)\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def mnist_noniid_unequal(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from MNIST dataset s.t clients\n",
        "    have unequal amount of data\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :returns a dict of clients with each clients assigned certain\n",
        "    number of training imgs\n",
        "    \"\"\"\n",
        "    # 60,000 training imgs --> 50 imgs/shard X 1200 shards\n",
        "    num_shards, num_imgs = 1200, 50\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    labels = dataset.train_labels.numpy()\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # Minimum and maximum shards assigned per client:\n",
        "    min_shard = 1\n",
        "    max_shard = 30\n",
        "\n",
        "    # Divide the shards into random chunks for every client\n",
        "    # s.t the sum of these chunks = num_shards\n",
        "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
        "                                          size=num_users)\n",
        "    random_shard_size = np.around(random_shard_size /\n",
        "                                  sum(random_shard_size) * num_shards)\n",
        "    random_shard_size = random_shard_size.astype(int)\n",
        "\n",
        "    # Assign the shards randomly to each client\n",
        "    if sum(random_shard_size) > num_shards:\n",
        "\n",
        "        for i in range(num_users):\n",
        "            # First assign each client 1 shard to ensure every client has\n",
        "            # atleast one shard of data\n",
        "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "        random_shard_size = random_shard_size-1\n",
        "\n",
        "        # Next, randomly assign the remaining shards\n",
        "        for i in range(num_users):\n",
        "            if len(idx_shard) == 0:\n",
        "                continue\n",
        "            shard_size = random_shard_size[i]\n",
        "            if shard_size > len(idx_shard):\n",
        "                shard_size = len(idx_shard)\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "    else:\n",
        "\n",
        "        for i in range(num_users):\n",
        "            shard_size = random_shard_size[i]\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[i] = np.concatenate(\n",
        "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "        if len(idx_shard) > 0:\n",
        "            # Add the leftover shards to the client with minimum images:\n",
        "            shard_size = len(idx_shard)\n",
        "            # Add the remaining shard to the client with lowest data\n",
        "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
        "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
        "                                            replace=False))\n",
        "            idx_shard = list(set(idx_shard) - rand_set)\n",
        "            for rand in rand_set:\n",
        "                dict_users[k] = np.concatenate(\n",
        "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
        "                    axis=0)\n",
        "\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def cifar_iid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample I.I.D. client data from CIFAR10 dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return: dict of image index\n",
        "    \"\"\"\n",
        "    num_items = int(len(dataset)/num_users)\n",
        "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
        "    for i in range(num_users):\n",
        "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
        "                                             replace=False))\n",
        "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "def cifar_noniid(dataset, num_users):\n",
        "    \"\"\"\n",
        "    Sample non-I.I.D client data from CIFAR10 dataset\n",
        "    :param dataset:\n",
        "    :param num_users:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_shards, num_imgs = 200, 250\n",
        "    idx_shard = [i for i in range(num_shards)]\n",
        "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
        "    idxs = np.arange(num_shards*num_imgs)\n",
        "    # labels = dataset.train_labels.numpy()\n",
        "    labels = np.array(dataset.train_labels)\n",
        "\n",
        "    # sort labels\n",
        "    idxs_labels = np.vstack((idxs, labels))\n",
        "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
        "    idxs = idxs_labels[0, :]\n",
        "\n",
        "    # divide and assign\n",
        "    for i in range(num_users):\n",
        "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
        "        idx_shard = list(set(idx_shard) - rand_set)\n",
        "        for rand in rand_set:\n",
        "            dict_users[i] = np.concatenate(\n",
        "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
        "    return dict_users\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def noniid_dirichlet(dataset, num_users, alpha):\n",
        "    \"\"\"\n",
        "    Sample non-IID client data from dataset using Dirichlet distribution\n",
        "    :param dataset: The dataset to sample from\n",
        "    :param num_users: Number of users/clients\n",
        "    :param alpha: Dirichlet distribution parameter\n",
        "    :return: Dictionary of client data indices\n",
        "    \"\"\"\n",
        "    num_samples = len(dataset)\n",
        "    class_counts = [0] * len(dataset.classes)  # Count of samples for each class\n",
        "\n",
        "    # Calculate total number of samples for each class\n",
        "    for i in range(num_samples):\n",
        "        _, label = dataset[i]\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    alpha_scaled = [ a * num_users for a in alpha ]\n",
        "    # Generate Dirichlet parameters based on alpha\n",
        "    dirichlet_params = np.random.dirichlet(alpha_scaled, num_users)\n",
        "\n",
        "    dict_users = {}\n",
        "    for user_id in range(num_users):\n",
        "        # Sample data proportion for each class for the user\n",
        "        user_data_proportion = dirichlet_params[user_id]\n",
        "\n",
        "        # Calculate the number of samples for each class based on the proportion\n",
        "        user_class_counts = [int(p * count) for p, count in zip(user_data_proportion, class_counts)]\n",
        "\n",
        "        # Assign data indices to the user based on the sampled counts\n",
        "        user_indices = []\n",
        "        for class_id, count in enumerate(user_class_counts):\n",
        "            class_indices = [i for i in range(num_samples) if dataset[i][1] == class_id]\n",
        "            user_indices.extend(np.random.choice(class_indices, count, replace=False))\n",
        "\n",
        "        dict_users[user_id] = user_indices\n",
        "\n",
        "    return dict_users\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVrA7tLQjINx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import copy\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "def get_dataset(args):\n",
        "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
        "    the keys are the user index and the values are the corresponding data for\n",
        "    each of those users.\n",
        "    \"\"\"\n",
        "\n",
        "    if args.dataset == 'cifar10':\n",
        "        data_dir = './data'\n",
        "        apply_transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "        train_dataset = datasets.CIFAR10(data_dir, train=True, download=True,\n",
        "                                       transform=apply_transform)\n",
        "\n",
        "        test_dataset = datasets.CIFAR10(data_dir, train=False, download=True,\n",
        "                                      transform=apply_transform)\n",
        "\n",
        "        # sample training data amongst users\n",
        "        if args.iid:\n",
        "            # Sample IID user data from Mnist\n",
        "            user_groups = cifar_iid(train_dataset, args.num_users)\n",
        "        else:\n",
        "            # Sample Non-IID user data from Mnist\n",
        "            if args.unequal:\n",
        "                # Chose uneuqal splits for every user\n",
        "                raise NotImplementedError()\n",
        "            else:\n",
        "                # Chose euqal splits for every user\n",
        "                user_groups = cifar_noniid(train_dataset, args.num_users)\n",
        "\n",
        "\n",
        "    elif args.dataset == 'mnist':\n",
        "        data_dir = './data'\n",
        "\n",
        "        apply_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "        train_dataset = datasets.MNIST(data_dir, train=True, download=True,\n",
        "                                       transform=apply_transform)\n",
        "\n",
        "        test_dataset = datasets.MNIST(data_dir, train=False, download=True,\n",
        "                                      transform=apply_transform)\n",
        "\n",
        "        # sample training data amongst users\n",
        "        if args.iid:\n",
        "            # Sample IID user data from Mnist\n",
        "            user_groups = mnist_iid(train_dataset, args.num_users)\n",
        "        else:\n",
        "            # Sample Non-IID user data from Mnist\n",
        "            if args.unequal:\n",
        "                # Chose uneuqal splits for every user\n",
        "                user_groups = mnist_noniid_unequal(train_dataset, args.num_users)\n",
        "            else:\n",
        "                # Chose euqal splits for every user\n",
        "                user_groups = mnist_noniid(train_dataset, args.num_users)\n",
        "\n",
        "\n",
        "    elif args.dataset == 'fmnist':\n",
        "        data_dir = './data'\n",
        "\n",
        "        apply_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "        train_dataset = datasets.FashionMNIST(data_dir, train=True, download=True,\n",
        "                                       transform=apply_transform)\n",
        "\n",
        "        test_dataset = datasets.FashionMNIST(data_dir, train=False, download=True,\n",
        "                                      transform=apply_transform)\n",
        "\n",
        "        # sample training data amongst users\n",
        "        if args.iid:\n",
        "            # Sample IID user data from Mnist\n",
        "            user_groups = mnist_iid(train_dataset, args.num_users)\n",
        "        else:\n",
        "            # Sample Non-IID user data from Mnist\n",
        "            if args.unequal:\n",
        "                # Chose uneuqal splits for every user\n",
        "                user_groups = mnist_noniid_unequal(train_dataset, args.num_users)\n",
        "            else:\n",
        "                # Chose euqal splits for every user\n",
        "                user_groups = mnist_noniid(train_dataset, args.num_users)\n",
        "\n",
        "    return train_dataset, test_dataset, user_groups\n",
        "\n",
        "\n",
        "def average_weights(w):\n",
        "    \"\"\"\n",
        "    Returns the average of the weights.\n",
        "    \"\"\"\n",
        "    w_avg = copy.deepcopy(w[0])\n",
        "    for key in w_avg.keys():\n",
        "        for i in range(1, len(w)):\n",
        "            w_avg[key] += w[i][key]\n",
        "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
        "    return w_avg\n",
        "\n",
        "\n",
        "def exp_details(args):\n",
        "    print('\\nExperimental details:')\n",
        "    print(f'    Model     : {args.model}')\n",
        "    print(f'    Dataset   : {args.dataset}')\n",
        "    print(f'    Optimizer : {args.optimizer}')\n",
        "    print(f'    Learning  : {args.lr}')\n",
        "    print(f'    Global Rounds   : {args.epochs}\\n')\n",
        "\n",
        "    print('    Federated parameters:')\n",
        "    if args.iid:\n",
        "        print('    IID')\n",
        "    else:\n",
        "        print('    Non-IID')\n",
        "    print(f'    Fraction of users  : {args.frac}')\n",
        "    print(f'    Local Batch size   : {args.local_bs}')\n",
        "    print(f'    Local Epochs       : {args.local_ep}\\n')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNcVcYtRmHG0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class DatasetSplit(Dataset):\n",
        "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = [int(i) for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return torch.tensor(image), torch.tensor(label)\n",
        "\n",
        "\n",
        "class LocalUpdate(object):\n",
        "    def __init__(self, args, dataset, idxs, logger):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "        self.trainloader, self.validloader, self.testloader = self.train_val_test(\n",
        "            dataset, list(idxs))\n",
        "        #self.device = 'cuda' if args.gpu else 'cpu'  #########################################CHANGE THIS WHEN ADDING A GPU\n",
        "        self.device = 'cpu'\n",
        "        # Default criterion set to NLL loss function\n",
        "        self.criterion = nn.NLLLoss().to(self.device)\n",
        "\n",
        "    def train_val_test(self, dataset, idxs):\n",
        "        \"\"\"\n",
        "        Returns train, validation and test dataloaders for a given dataset\n",
        "        and user indexes.\n",
        "        \"\"\"\n",
        "        # split indexes for train, validation, and test (80, 10, 10)\n",
        "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
        "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
        "        idxs_test = idxs[int(0.9*len(idxs)):]\n",
        "\n",
        "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
        "                                 batch_size=self.args.local_bs, shuffle=True)\n",
        "        validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
        "                                 batch_size=int(len(idxs_val)/10), shuffle=False)\n",
        "        testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
        "                                batch_size=int(len(idxs_test)/10), shuffle=False)\n",
        "        return trainloader, validloader, testloader\n",
        "\n",
        "\n",
        "    def update_weights(self, model, global_round):\n",
        "        # Set mode to train model\n",
        "        model.train()\n",
        "        epoch_loss = []\n",
        "\n",
        "        # Set optimizer for the local updates\n",
        "        if self.args.optimizer == 'sgd':\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=self.args.lr,\n",
        "                                        momentum=0.99)\n",
        "        elif self.args.optimizer == 'adam':\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=self.args.lr,\n",
        "                                         weight_decay=1e-6)\n",
        "\n",
        "        for iter in range(self.args.local_ep):\n",
        "            batch_loss = []\n",
        "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                model.zero_grad()\n",
        "                log_probs = model(images)\n",
        "                loss = self.criterion(log_probs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if self.args.verbose and (batch_idx % 10 == 0):\n",
        "                    print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                        global_round, iter, batch_idx * len(images),\n",
        "                        len(self.trainloader.dataset),\n",
        "                        100. * batch_idx / len(self.trainloader), loss.item()))\n",
        "                self.logger.add_scalar('loss', loss.item())\n",
        "                batch_loss.append(loss.item())\n",
        "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
        "\n",
        "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
        "\n",
        "    def inference(self, model):\n",
        "        \"\"\" Returns the inference accuracy and loss.\n",
        "        \"\"\"\n",
        "\n",
        "        model.eval()\n",
        "        loss, total, correct = 0.0, 0.0, 0.0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(self.testloader):\n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "            # Inference\n",
        "            outputs = model(images)\n",
        "            batch_loss = self.criterion(outputs, labels)\n",
        "            loss += batch_loss.item()\n",
        "\n",
        "            # Prediction\n",
        "            _, pred_labels = torch.max(outputs, 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "            total += len(labels)\n",
        "\n",
        "        accuracy = correct/total\n",
        "        return accuracy, loss\n",
        "\n",
        "\n",
        "def test_inference(args, model, test_dataset):\n",
        "    \"\"\" Returns the test accuracy and loss.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    loss, total, correct = 0.0, 0.0, 0.0\n",
        "\n",
        "    #device = 'cuda' if args.gpu else 'cpu' ####################################WHEN GPU IS INSTALLED\n",
        "    device = 'cpu'\n",
        "    criterion = nn.NLLLoss().to(device)\n",
        "    testloader = DataLoader(test_dataset, batch_size=128,\n",
        "                            shuffle=False)\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(testloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Inference\n",
        "        outputs = model(images)\n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        loss += batch_loss.item()\n",
        "\n",
        "        # Prediction\n",
        "        _, pred_labels = torch.max(outputs, 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "\n",
        "    accuracy = correct/total\n",
        "    return accuracy, loss\n",
        "\n",
        "\n",
        "\n",
        "class calculate(object):\n",
        "    def __init__(self, args, dataset, idxs, logger):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "        self.trainloader, self.validloader, self.testloader, self.sample_size = self.train_val_test(\n",
        "            dataset, list(idxs))\n",
        "        # self.device = 'cuda' if args.gpu else 'cpu'\n",
        "        self.device = 'cpu'\n",
        "        # Default criterion set to NLL loss function\n",
        "        self.criterion = nn.NLLLoss().to(self.device)\n",
        "\n",
        "    def train_val_test(self, dataset, idxs):\n",
        "        \"\"\"\n",
        "        Returns train, validation and test dataloaders for a given dataset\n",
        "        and user indexes.\n",
        "        \"\"\"\n",
        "        # split indexes for train, validation, and test (80, 10, 10)\n",
        "        idxs_train = list(idxs)[:int(0.8*len(idxs))]\n",
        "        idxs_val = list(idxs)[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
        "        idxs_test = list(idxs)[int(0.9*len(idxs)):]\n",
        "\n",
        "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
        "                                 batch_size=self.args.local_bs, shuffle=True)\n",
        "        validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
        "                                 batch_size=int(len(idxs_val)/10), shuffle=False)\n",
        "        testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
        "                                batch_size=int(len(idxs_test)/10), shuffle=False)\n",
        "        sample_size = len(trainloader.dataset)\n",
        "        return trainloader, validloader, testloader, sample_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCL4r6qgmo81"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_input = nn.Linear(dim_in, dim_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
        "        x = self.layer_input(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer_hidden(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "class CNNMnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNMnist, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class CNNFashion_Mnist(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNFashion_Mnist, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc = nn.Linear(7*7*32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNNCifar(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CNNCifar, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, args.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class modelC(nn.Module):\n",
        "    def __init__(self, input_size, n_classes=10, **kwargs):\n",
        "        super(AllConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, 96, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(96, 96, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(96, 96, 3, padding=1, stride=2)\n",
        "        self.conv4 = nn.Conv2d(96, 192, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(192, 192, 3, padding=1, stride=2)\n",
        "        self.conv7 = nn.Conv2d(192, 192, 3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(192, 192, 1)\n",
        "\n",
        "        self.class_conv = nn.Conv2d(192, n_classes, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_drop = F.dropout(x, .2)\n",
        "        conv1_out = F.relu(self.conv1(x_drop))\n",
        "        conv2_out = F.relu(self.conv2(conv1_out))\n",
        "        conv3_out = F.relu(self.conv3(conv2_out))\n",
        "        conv3_out_drop = F.dropout(conv3_out, .5)\n",
        "        conv4_out = F.relu(self.conv4(conv3_out_drop))\n",
        "        conv5_out = F.relu(self.conv5(conv4_out))\n",
        "        conv6_out = F.relu(self.conv6(conv5_out))\n",
        "        conv6_out_drop = F.dropout(conv6_out, .5)\n",
        "        conv7_out = F.relu(self.conv7(conv6_out_drop))\n",
        "        conv8_out = F.relu(self.conv8(conv7_out))\n",
        "\n",
        "        class_out = F.relu(self.class_conv(conv8_out))\n",
        "        pool_out = F.adaptive_avg_pool2d(class_out, 1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        pool_out.squeeze_(-1)\n",
        "        return pool_out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# building a ResNet18 Architecture\n",
        "#this function creates a 3x3 convolutional layer with specified input and output channels, stride, padding, and without bias\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X1NAAcuDnUbo",
        "outputId": "1c59d30b-790b-4b85-f0ea-0a9d6df9ebbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experimental details:\n",
            "    Model     : mlp\n",
            "    Dataset   : mnist\n",
            "    Optimizer : adam\n",
            "    Learning  : 0.1\n",
            "    Global Rounds   : 10\n",
            "\n",
            "    Federated parameters:\n",
            "    Non-IID\n",
            "    Fraction of users  : 0.1\n",
            "    Local Batch size   : 64\n",
            "    Local Epochs       : 2\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 103972029.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 19233993.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32202884.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14377757.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "<ipython-input-5-6061b20c5cc6>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(image), torch.tensor(label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layer_input): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (layer_hidden): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "the new learning rate\n",
            "0.099\n",
            "\n",
            " | Global Training Round : 1 |\n",
            "\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.113770\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.359375\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.083297\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.578125\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.090082\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.734375\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.110116\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.359375\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.095906\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.671875\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.070353\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -1.000000\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.111846\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.484375\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.119612\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.672116\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.103420\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.468750\n",
            "| Global Round : 0 | Local Epoch : 0 | [0/480 (0%)]\tLoss: -0.106505\n",
            "| Global Round : 0 | Local Epoch : 1 | [0/480 (0%)]\tLoss: -0.577953\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-22d5060e2b6c>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         local_model = LocalUpdate(args=args, dataset=train_dataset,\n\u001b[1;32m    106\u001b[0m                                   idxs=user_groups[idx], logger=logger)\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mlist_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mlist_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6061b20c5cc6>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# define paths\n",
        "path_project = os.path.abspath('..')\n",
        "logger = SummaryWriter('../logs')\n",
        "\n",
        "args = args_parser()\n",
        "exp_details(args)\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "# load dataset and user groups\n",
        "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
        "\n",
        "# BUILD MODEL\n",
        "if args.model == 'cnn':\n",
        "    # Convolutional neural netork\n",
        "    if args.dataset == 'mnist':\n",
        "        global_model = CNNMnist(args=args)\n",
        "    elif args.dataset == 'fmnist':\n",
        "        global_model = CNNFashion_Mnist(args=args)\n",
        "    elif args.dataset == 'cifar10':\n",
        "        global_model = CNNCifar(args=args)\n",
        "\n",
        "\n",
        "elif args.model == 'mlp':\n",
        "    # Multi-layer preceptron\n",
        "    img_size = train_dataset[0][0].shape\n",
        "    len_in = 1\n",
        "    for x in img_size:\n",
        "        len_in *= x\n",
        "        global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
        "                            dim_out=args.num_classes)\n",
        "elif args.model == 'resnet':\n",
        "    global_model = ResNet18(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "\n",
        "else:\n",
        "    exit('Error: unrecognized model')\n",
        "\n",
        "# Set the model to train and send it to device.\n",
        "global_model.to(device)\n",
        "global_model.train()\n",
        "print(global_model)\n",
        "\n",
        "# copy weights\n",
        "global_weights = global_model.state_dict()\n",
        "\n",
        "# Training\n",
        "train_loss, train_accuracy = [], []\n",
        "val_acc_list, net_list = [], []\n",
        "cv_loss, cv_acc = [], []\n",
        "print_every = 2\n",
        "val_loss_pre, counter = 0, 0\n",
        "\n",
        "\n",
        "initial_loss = random.sample(range(args.num_users), args.num_users)\n",
        "users_pool = np.random.choice(range(args.num_users), args.num_users, replace= False)\n",
        "users_pool.sort()\n",
        "\n",
        "weight_coefficients_p = []\n",
        "\n",
        "for idx in users_pool:\n",
        "  local_model = calculate(args = args, dataset= train_dataset, idxs = user_groups[idx], logger = logger)\n",
        "  a, b, d, c = local_model.train_val_test(dataset= train_dataset, idxs=user_groups[idx])\n",
        "  weight_coefficients_p.append(c)\n",
        "\n",
        "total_size=sum(weight_coefficients_p)\n",
        "weight_coefficients_p = [number/ total_size for number in weight_coefficients_p]\n",
        "\n",
        "\n",
        "# for epoch in tqdm(range(args.epochs)):\n",
        "for epoch in range(args.epochs):\n",
        "    args.lr *= 0.99\n",
        "    print(\"the new learning rate\")\n",
        "    print(args.lr)\n",
        "    local_weights, local_losses = [], []\n",
        "    print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
        "\n",
        "    global_model.train()\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False, p=weight_coefficients_p)\n",
        "\n",
        "    for idx in idxs_users:\n",
        "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
        "                                  idxs=user_groups[idx], logger=logger)\n",
        "        w, loss = local_model.update_weights(\n",
        "            model=copy.deepcopy(global_model), global_round=epoch)\n",
        "        local_weights.append(copy.deepcopy(w))\n",
        "        local_losses.append(copy.deepcopy(loss))\n",
        "\n",
        "    # update global weights\n",
        "    global_weights = average_weights(local_weights)\n",
        "\n",
        "    # update global weights\n",
        "    global_model.load_state_dict(global_weights)\n",
        "\n",
        "    loss_avg = sum(local_losses) / len(local_losses)\n",
        "    train_loss.append(loss_avg)\n",
        "\n",
        "    # Calculate avg training accuracy over all users at every epoch\n",
        "    list_acc, list_loss = [], []\n",
        "    global_model.eval()\n",
        "    for c in range(args.num_users):\n",
        "        local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
        "                                  idxs=user_groups[idx], logger=logger)\n",
        "        acc, loss = local_model.inference(model=global_model)\n",
        "        list_acc.append(acc)\n",
        "        list_loss.append(loss)\n",
        "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
        "\n",
        "    # print global training loss after every 'i' rounds\n",
        "    if (epoch+1) % print_every == 0:\n",
        "        print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
        "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
        "        print('Train Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))\n",
        "\n",
        "# Test inference after completion of training\n",
        "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
        "\n",
        "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
        "print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
        "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
        "\n",
        "\n",
        "\n",
        "print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpxXDA947I8L"
      },
      "outputs": [],
      "source": [
        "#print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AedUpKuGtbCI"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "plt.ion()\n",
        "plt.figure()\n",
        "plt.plot(range(len(train_accuracy)), train_accuracy)\n",
        "plt.title('Training accuracy vs communication rounds')\n",
        "plt.ylabel('Training accuracy')\n",
        "plt.xlabel('Communication round')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"/content/train_accuracy.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bspzjGum6x0V"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/train_accuracy.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJhLn0Ju72gx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}